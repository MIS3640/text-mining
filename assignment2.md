# Assignment 2 

## Team members: James Lee, Emily Muramoto, Lina Li 

### Project Overview
For this project, we utilized two sources, a CSV file that contains all the US traded stocks, and the importation of the top 100 reddit titles under r/wallstreetbets. We processed the CSV file by appending every stock ticker into a list. We processed the reddit titles by cross referencing each word in each title against the stock ticker list to identify which stock the post would target. We analyzed the data using a frequency analysis, hoping to identify the trending stocks in r/wallstreetbets. 

### Implementation 
Our first course of action was to upload a csv file that contained all of the stock tickers —  and other information about the stock — traded in the United States. We created a function to compile all of the stock tickers into a list, later used to identify whether or not a word is a stock ticker. Afterwards, we acquired the top 100 post titles from r/wallstreetbets and compared the words in each title against our list of stock tickers to identify if the post is referencing any specific stocks. We counted the frequency of post titles that a stock would appear in and implemented it into a dictionary — with the key as the ticker and the value as the frequency. Next, we sorted it by descending order to identify which stocks are currently trending on the subreddit. Lastly, we compiled our findings into a bar graph to create a visual representation, in order to help with the analysis. 


One of the design decisions that we had to make that had multiple alternatives was the compilation of the stock tickers into a list. That function could have returned either a dictionary or a list, as the final usage was to see if a word from the reddit post title was in the list. However, we opted to utilize a list instead of a dictionary, because we did not have a value to assign to the stock ticker. If we were to use a dictionary, the values would have been a random assignment with no meaning. 

### Results
From this project, we were able to find the stocks that were mentioned the most in the subreddit. As you can see from the bar graph below, GME (Gamestop) and AMC (AMC Entertainment Holdings) were mentioned the most, which was unsuprising given the recent news that this specific subreddit was responsible for their increase in stock value. Besides these two, it was interesting to see the other stocks that were mentioned in the subreddit, including BB (Blackberry) and NOK (Nokia). With this information, we could analyze and see which stock the subreddit is planning on targeting next by seeing what stock is also being talked about more. 


![161523891_349782642999425_7494971577313862557_n](https://user-images.githubusercontent.com/77700744/111401041-e6332b80-869e-11eb-9a5d-63de52bd9b9b.png)

### Reflection

From a process point of view, we were successful in accomplishing our goal of importing different types of data sources into Python using different techniques. On the other hand, one aspect of our code that could be improved is the run time in respect to the number of Reddit submissions. Currently, our code, containing 100 submissions, has a run time of approximately 30 minutes. Thus if we were to increase the number of submissions to 10,000 for example, it would take our code approximately 50 hours to run, a considerably unreasonable run time for our project. Reddit also has a limit to the number of posts that a user can request in their API, somewhere between the range of 200 and 300 posts. Additionally, in the beginning stages of the project, we bulk coded everything; however, we soon realized that it was difficult to debug and find errors. Thus, we switched our methods and began unit testing each function in the main() function. Going forward, we will continue to implement unit testing to ensure that each function runs appropriately before moving onto the next step. In terms of what we wished we knew before we started, it would have been beneficial to know a technique to filter unwanted ticker counts. To elaborate, for tickers such as "I" and "A," mentions are more often than not related to the pronoun "I" and the grammar determiner "A," and since we were unsure of how to isolate these, we opted to omit these tickers in our analysis.

For our team process, we planned and also executed the work by task. For the code, one member, James, served as the driver, while the remaining two members, Emily and Lina, served as the navigators. To further balance the workload between driver and navigator, Emily and Lina took an active role in the write-up, while James helped with general overview and editing. Moreover, for any questions that the navigators could not address, including how to filter unwanted tickers (e.g "I"), we met with Professor Li. In terms of the coding process, there were no issues that arose while working together. For next time, we would like to rotate our roles as driver and navigators to ensure that we all have an equal workload and opportunity. 
