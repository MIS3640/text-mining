## Project Overview: 
    This project consists of data sourced from Wikipedia analyzed with an  abundance of for loops. Through this process, our goal was to create code that would provide the differences in links and words between two different Wikipedia pages to estimate how similar two Wiki pages are. Moreover, we wanted to find the n most common words shard between the articles and all the common links used between both articles. This program is designed to be a helper for a user playing the "Wikipedia Game" to show them unique connections between two articles. 




## Implementation:
    ## Explain how the major components, algorithms, and data structures fit together. Discuss one design decision where we chose between several alternatives and why.
    The major compontents of this code are the text analysis, data sourcing and the comparison/analysis of links. Our first big decision came when we were tasked with gathering the data from Wikipedia and trying to store it. We first decided to store it in a text file, which worked brilliantly until it didn't. Putting the content text into a text file just to take it out was inefficient, and confusing. We decided to store the wiki pages directly as strings to simplify it as much as possible. Making this decision took a lot of time because we were committed to making the text file storage work but it was easy to make after a few hours of banging our head against the code, strings were gonna be way easier to work with based on our current coding experience.

    By first storing the article content as a string, we were able to process the individuals words easily by sorting them into a dictionary. Our word analysis was based upon using for loops to iterate through the dictionary we created. Since this project is based around comparison, our team ended up relying heavily on nested for loops to iterate through two dictionaries at the same time to find matching values. Additinoally, since the links and backlinks were returned to us in the form of lists, lots of our functions contain nested for loops to iterate through two lists at the same time. 



## Results: 
    The result of the text analysis was not so interesting for the majority of wikipedia articles, as the vast majority of articles follow an extremely similar word frequency distribution to English. The most common 5 words were almost always the same, "the, of, to, and, a". So comparing the most common words was not so fruitful because it makes sense that two articles in English will have similar word distributions. However, the more similar the articles got, the more likely it was for a word not within the most frequent English words to appear. For example, when comparing the Wiki articles "Babson" and "MIT", the 7 most common words shared between the two articles are all boring, frequent words in the English language. Interestingly however, the 8th most common word shared was 'student', a word that wouldn't even be top 100 in the frequencies of English. So while most Wikipedia articles follow traditional English word distributions, smaller and more specific articles are more prone to diverge from this tradition. 

    Our most interesting result was comparing the backlinks to see how many articles reference the two pages at once. This was really interesting when comparing any article to find bizzare connections between two topics. Take for example "Babson" and "NASA". One of the backlinks these two pages share is an article titled "anti-gravity'. What does Babson College have to do with anti-gravity? Turns out Roger Babson formed a foundation to study ways to reduce the effect of gravity. Finding these bizzarre links between articles was super interesting but not entirely super useful. These backlinks provided very interesting insights into the interconnectivity of Wikipedia, but did not contribute to a better understanding of the real world. 

    Another interesting result was the sentiment analysis. Even though Wikipedia is dedicated to being an objective source of information, millions of users contribute to the site. Because of this, our group was curious to see if there would be any major difference in sentiment based on article content. It turns out that there is indeed a difference in sentiment based on the article content. Take for example the article "Happiness" and the article "Death". Ideally, if our hypothesis was right, there would be a difference in sentiment between these two articles. Our code shows that there is indeed a significant difference in the sentiment scores for negativity and positivity, which shows that the article subject matter plays a role in determining the sentiment of the article. However, we were unable to determine the individuals user's influence on the sentiment. Without controlling for this, the group is unsure of the validity of this result, however we believe it to be a close approximation of the real verdict. 
## Reflection
    In general, the assignment went smoothly after the data storage fiasco was sorted out. Getting our hands on the raw data was made much more complicated by trying too many unecessary things, and we ended up wasting a lot of time trying to do that. The pair coding was fun and eventually worked well to bounce ideas off one another. However, working together through github was extremely confusing. Our pair has a lack of understanding for github so we just ended up sharing the code in primitive ways, like copy and pasting it to one another. Going forward after this assingment we have commited ourselves to learning how to use github more effectively and efficiently. Something that would've helped our group complete this assignment to a higher degree of proficiency would be more motivation to start this project earlier, and a better understanding of github. In terms of team process, our team worked well together and never struggled to schedule a meeting. One issue that arose while pair programming was when one of us would have a question for the coder, we would have to pull eachother out of the zone and break the concentration of one another. For some reason, we both found it more difficult to code quickly while working in pairs, even typing while being watched by someone became more difficult, and we were both prone to making more errors while even typing into questions into google. These issues ended up not being a real problem once we got more comfortable working with one another and we were able to adopt a more comfortable workflow. 